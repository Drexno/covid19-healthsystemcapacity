{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge facility information\n",
    "\n",
    "Merge facility data from HCRIS (Healthcare Cost Reporting Information System) and DH (Definitive Healthcare) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from os.path import join, isdir\n",
    "\n",
    "from covidcaremap.geo import spatial_join_facilities\n",
    "from covidcaremap.data import (processed_data_path, \n",
    "                               external_data_path,\n",
    "                               local_data_path)\n",
    "from covidcaremap.mapping import HospMap\n",
    "from covidcaremap.merge import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geocoded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcris = gpd.read_file(processed_data_path('usa_facilities_hcris_geocoded.geojson'), encoding='utf-8')\n",
    "dh = gpd.read_file(processed_data_path('dh_geocoded_v1_0326202.geojson'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns to match between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcris.rename(columns={\n",
    "    'ST_ABBR': 'STATE_NAME',\n",
    "    'Zip_Code': 'ZIP_CODE'\n",
    "}, inplace=True)\n",
    "\n",
    "dh['STATE_NAME'] = dh['ST_ABBR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of states to iterate over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = hcris['STATE_NAME'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect data set, data set name, and data set uid for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcris_info = (hcris, 'HCRIS', 'Provider Number')\n",
    "dh_info = (dh, 'DH', 'OBJECTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_by_state(facility_info_1, facility_info_2, map_dir=None, str_match_method='name'):\n",
    "    df1, name1, uid1 = facility_info_1\n",
    "    df2, name2, uid2 = facility_info_2\n",
    "    \n",
    "    if not os.path.isdir(processed_data_path(map_dir)):\n",
    "        os.makedirs(processed_data_path(map_dir))\n",
    "    \n",
    "    state_matches = {}\n",
    "    for state in all_states:\n",
    "        print('Matching facilities in {}'.format(state))\n",
    "        if state not in state_matches.keys():\n",
    "            df1_s = df1[df1['STATE_NAME'] == state].reset_index().copy()\n",
    "            df2_s = df2[df2['STATE_NAME'] == state].reset_index().copy()\n",
    "            m = Matcher(df1_s, df2_s, uid1, uid2)\n",
    "            m.match_point_set((100, 500), n=10, str_match_method=str_match_method)\n",
    "            if map_dir:\n",
    "                all_map = m.map_all((name1, name2), ['match source', 'dist_apart'])\n",
    "                all_map.add_layer_selector()\n",
    "                all_map.save(join(processed_data_path('{}'.format(map_dir)), '{}.html'.format(state)))\n",
    "            state_matches[state] = m\n",
    "    \n",
    "    ds = {\n",
    "        f'{name1}_matched': [],\n",
    "        f'{name2}_matched': [],\n",
    "        f'{name1}_unmatched': [],\n",
    "        f'{name2}_unmatched': [],\n",
    "        'matching_dfs': []\n",
    "    }\n",
    "    \n",
    "    for _, v in state_matches.items():\n",
    "        ds[f'{name1}_matched'].append(v.d1_matched)\n",
    "        ds[f'{name2}_matched'].append(v.d2_matched)\n",
    "        ds[f'{name1}_unmatched'].append(v.d1_unmatched)\n",
    "        ds[f'{name2}_unmatched'].append(v.d2_unmatched)\n",
    "        ds['matching_dfs'].append(v.matching_key_df())\n",
    "    \n",
    "    for k, v in ds.items():\n",
    "        ds[k] = pd.concat(v)\n",
    "        if isinstance(ds[k], gpd.GeoDataFrame):\n",
    "            ds[k] = ds[k].to_crs('epsg:4326')\n",
    "    \n",
    "    print('------------')\n",
    "    n_matched = len(ds['matching_dfs'])\n",
    "    n_unmatched = len(ds[f'{name1}_unmatched'])\n",
    "    n_total = n_matched + n_unmatched\n",
    "    pct_matched = round((n_matched / n_total) * 100, 1)\n",
    "    print(f'{name1} to {name2} matches: {pct_matched}% ({n_matched} of {n_total})')\n",
    "    \n",
    "    return state_matches, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the two data sets state by state, writing off folium maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcris_to_dh_matches, hcris_to_dh_data = match_by_state(\n",
    "    hcris_info, \n",
    "    dh_info, \n",
    "    'state_validation_maps_03-31-21_hcris-to-dh')\n",
    "\n",
    "HtoD_matches = hcris_to_dh_data['matching_dfs'].astype(str)\n",
    "HtoD_matches.to_csv(local_data_path('HCRIS_to_DH_matching_key.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine into one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some field names changed in the process of geocoding so we need to import the original datasets and join them using the newly created matching key to ensure that column names are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcris_original = gpd.read_file(processed_data_path('usa_hospital_beds_hcris2018.geojson'), encoding='utf-8')\n",
    "hcris_original['Provider Number'] = hcris_original['Provider Number'].astype(str)\n",
    "dh_original = gpd.read_file(external_data_path('dh_facility_data.geojson'), encoding='utf-8')\n",
    "dh_original['OBJECTID'] = dh_original['OBJECTID'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `Provider Number` (HCRIS unique ID field) to the DH dataset by joining the key to it on `OBJECTID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_with_pn = pd.merge(dh_original, HtoD_matches[['OBJECTID', 'Provider Number']], how='left', on='OBJECTID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then join the HCRIS dataset to the updated DH dataset on `Provider Number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(dh_with_pn, hcris_original.drop(columns='geometry'), how='left', on='Provider Number', suffixes=('', '_HCRIS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the necessary columns by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['OBJECTID', 'Provider Number', 'HOSPITAL_N', 'HOSPITAL_T', 'HQ_ADDRESS',\n",
    "        'HQ_ADDRE_1', 'HQ_CITY', 'HQ_STATE', 'HQ_ZIP_COD', 'COUNTY_NAM',\n",
    "        'STATE_NAME', 'STATE_FIPS', 'CNTY_FIPS', 'FIPS', 'NUM_LICENS',\n",
    "        'NUM_STAFFE', 'NUM_ICU_BE', 'BED_UTILIZ', 'Potential_', 'FYB', 'FYE',\n",
    "        'STATUS', 'CTRL_TYPE', 'HOSP10_Name', 'Street_Addr', 'PO_Box', 'City',\n",
    "        'State', 'Zip_Code', 'County', 'Hospital Adult and Peds Staffed Beds',\n",
    "        'Hospital Adult and Peds Bed Days Available',\n",
    "        'Hospital Adult and Peds Inpatient Days',\n",
    "        'Intensive Care Unit Staffed Beds',\n",
    "        'Intensive Care Unit Bed Days Available',\n",
    "        'Intensive Care Unit Inpatient Days', 'Coronary Care Unit Staffed Beds',\n",
    "        'Coronary Care Unit Bed Days Available',\n",
    "        'Coronary Care Unit Inpatient Days', 'Burn ICU Staffed Beds',\n",
    "        'Burn ICU Bed Days Available', 'Burn ICU Inpatient Days',\n",
    "        'Surgical ICU Staffed Beds', 'Surgical ICU Bed Days Available',\n",
    "        'Surgical ICU Inpatient Days', 'Total Staffed Beds',\n",
    "        'Total Bed Days Available', 'Total Inpatient Days',\n",
    "        'ICU Total Staffed Beds', 'ICU Total Bed Days Available',\n",
    "        'ICU Total Inpatient Days', 'ICU Occupancy Rate',\n",
    "        'Total Bed Occupancy Rate', 'geometry']\n",
    "\n",
    "full_df = full_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write off the merged geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_file(processed_data_path('dh_hcris_merged_facility_data.geojson'), \n",
    "                encoding='utf-8', \n",
    "                driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
